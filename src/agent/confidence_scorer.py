"""
Confidence Scoring System for Decision-Making Agent

This module provides comprehensive confidence scoring for responses
generated by different strategies (RAG, web search, direct answer).
"""

import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import numpy as np
from datetime import datetime

from .query_classifier import QueryClassification, ResponseStrategy

logger = logging.getLogger(__name__)


@dataclass
class ConfidenceFactors:
    """Data class for confidence factors."""
    query_confidence: float
    retrieval_confidence: float
    source_reliability: float
    answer_completeness: float
    temporal_relevance: float
    domain_alignment: float
    consistency_score: float


@dataclass
class ConfidenceScore:
    """Data class for final confidence score."""
    overall_score: float
    factors: ConfidenceFactors
    breakdown: Dict[str, float]
    recommendations: List[str]
    strategy: str


class ConfidenceScorer:
    """
    Comprehensive confidence scoring system for response quality assessment.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize confidence scorer.
        
        Args:
            config: Configuration dictionary with scoring weights
        """
        self.config = config or {}
        
        # Weight configuration for different factors
        self.weights = {
            'query_confidence': 0.15,
            'retrieval_confidence': 0.25,
            'source_reliability': 0.20,
            'answer_completeness': 0.15,
            'temporal_relevance': 0.10,
            'domain_alignment': 0.10,
            'consistency_score': 0.05
        }
        
        # Update weights from config if provided
        if 'weights' in self.config:
            self.weights.update(self.config['weights'])
    
    def calculate_confidence(
        self,
        query_classification: QueryClassification,
        response_data: Dict[str, Any],
        strategy: ResponseStrategy
    ) -> ConfidenceScore:
        """
        Calculate comprehensive confidence score for a response.
        
        Args:
            query_classification: Query classification results
            response_data: Response data from the chosen strategy
            strategy: Response strategy used
            
        Returns:
            ConfidenceScore with detailed analysis
        """
        # Calculate individual confidence factors
        factors = ConfidenceFactors(
            query_confidence=self._calculate_query_confidence(query_classification),
            retrieval_confidence=self._calculate_retrieval_confidence(response_data, strategy),
            source_reliability=self._calculate_source_reliability(response_data, strategy),
            answer_completeness=self._calculate_answer_completeness(response_data),
            temporal_relevance=self._calculate_temporal_relevance(response_data, strategy),
            domain_alignment=self._calculate_domain_alignment(query_classification, response_data),
            consistency_score=self._calculate_consistency_score(response_data)
        )
        
        # Calculate weighted overall score
        overall_score = self._calculate_weighted_score(factors)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(factors, overall_score)
        
        # Create breakdown
        breakdown = {
            'query_confidence': factors.query_confidence,
            'retrieval_confidence': factors.retrieval_confidence,
            'source_reliability': factors.source_reliability,
            'answer_completeness': factors.answer_completeness,
            'temporal_relevance': factors.temporal_relevance,
            'domain_alignment': factors.domain_alignment,
            'consistency_score': factors.consistency_score
        }
        
        return ConfidenceScore(
            overall_score=overall_score,
            factors=factors,
            breakdown=breakdown,
            recommendations=recommendations,
            strategy=strategy.value
        )
    
    def _calculate_query_confidence(
        self,
        query_classification: QueryClassification
    ) -> float:
        """Calculate confidence based on query classification."""
        base_confidence = query_classification.confidence
        
        # Adjust based on query type
        type_adjustments = {
            'factual': 0.1,
            'definition': 0.05,
            'analytical': -0.05,
            'procedural': 0.05,
            'comparative': -0.1,
            'temporal': -0.15,
            'opinion': -0.2,
            'general': -0.1
        }
        
        adjustment = type_adjustments.get(query_classification.query_type.value, 0)
        adjusted_confidence = base_confidence + adjustment
        
        # Boost for domain-specific queries
        if query_classification.domain_specific:
            adjusted_confidence += 0.15
        
        return max(0.0, min(adjusted_confidence, 1.0))
    
    def _calculate_retrieval_confidence(
        self,
        response_data: Dict[str, Any],
        strategy: ResponseStrategy
    ) -> float:
        """Calculate confidence based on retrieval quality."""
        if strategy == ResponseStrategy.RAG_RETRIEVAL:
            return response_data.get('confidence', 0.0)
        
        elif strategy == ResponseStrategy.WEB_SEARCH:
            # For web search, assess result quality
            results = response_data.get('results', [])
            if not results:
                return 0.0
            
            # Calculate based on relevance scores and source diversity
            relevance_scores = [r.get('relevance_score', 0.7) for r in results]
            avg_relevance = np.mean(relevance_scores)
            
            # Source quality bonus
            high_quality_sources = ['oecd', 'irs', 'deloitte', 'kpmg', 'pwc', 'ey']
            source_bonus = 0.0
            for result in results:
                source = result.get('source', '').lower()
                if any(hq in source for hq in high_quality_sources):
                    source_bonus += 0.1
            
            return min(avg_relevance + source_bonus, 1.0)
        
        elif strategy == ResponseStrategy.DIRECT_ANSWER:
            # For direct answers, base on query confidence
            return 0.8  # High confidence for simple queries
        
        else:  # HYBRID
            # Average of RAG and web search confidences
            rag_conf = response_data.get('rag_confidence', 0.0)
            web_conf = response_data.get('web_confidence', 0.0)
            return (rag_conf + web_conf) / 2
    
    def _calculate_source_reliability(
        self,
        response_data: Dict[str, Any],
        strategy: ResponseStrategy
    ) -> float:
        """Calculate source reliability score."""
        if strategy == ResponseStrategy.RAG_RETRIEVAL:
            # RAG sources are from processed documents
            sources = response_data.get('sources', [])
            if not sources:
                return 0.5
            
            # All RAG sources are considered reliable (processed OECD documents)
            return 0.9
        
        elif strategy == ResponseStrategy.WEB_SEARCH:
            # Web sources need reliability assessment
            results = response_data.get('results', [])
            if not results:
                return 0.3
            
            # Define source reliability tiers
            tier1_sources = {
                'oecd.org': 1.0,
                'irs.gov': 1.0,
                'treasury.gov': 1.0,
                'ec.europa.eu': 0.95
            }
            
            tier2_sources = {
                'deloitte.com': 0.9,
                'kpmg.com': 0.9,
                'pwc.com': 0.9,
                'ey.com': 0.9,
                'taxfoundation.org': 0.85
            }
            
            reliability_scores = []
            for result in results:
                url = result.get('url', '').lower()
                source = result.get('source', '').lower()
                
                score = 0.5  # Default for unknown sources
                
                # Check tier 1
                for domain, reliability in tier1_sources.items():
                    if domain in url or domain in source:
                        score = reliability
                        break
                
                # Check tier 2
                if score == 0.5:
                    for domain, reliability in tier2_sources.items():
                        if domain in url or domain in source:
                            score = reliability
                            break
                
                reliability_scores.append(score)
            
            return np.mean(reliability_scores) if reliability_scores else 0.5
        
        else:
            return 0.7  # Default for direct answers
    
    def _calculate_answer_completeness(
        self,
        response_data: Dict[str, Any]
    ) -> float:
        """Calculate answer completeness score."""
        answer = response_data.get('answer', '')
        if not answer:
            return 0.0
        
        # Length-based completeness
        word_count = len(answer.split())
        
        if word_count < 10:
            return 0.3
        elif word_count < 50:
            return 0.6
        elif word_count < 100:
            return 0.8
        else:
            return 0.9
        
        # Additional checks for specific completeness indicators
        completeness_indicators = [
            'because', 'therefore', 'however', 'specifically',
            'for example', 'in particular', 'according to'
        ]
        
        indicator_bonus = sum(0.1 for indicator in completeness_indicators 
                            if indicator in answer.lower())
        
        return min(0.9 + indicator_bonus, 1.0)
    
    def _calculate_temporal_relevance(
        self,
        response_data: Dict[str, Any],
        strategy: ResponseStrategy
    ) -> float:
        """Calculate temporal relevance score."""
        current_year = datetime.now().year
        
        if strategy == ResponseStrategy.RAG_RETRIEVAL:
            # Check document dates
            sources = response_data.get('sources', [])
            if not sources:
                return 0.7
            
            # Assume documents are recent (processed OECD documents)
            return 0.85
        
        elif strategy == ResponseStrategy.WEB_SEARCH:
            # Check result dates
            results = response_data.get('results', [])
            if not results:
                return 0.5
            
            recent_count = 0
            for result in results:
                date_str = result.get('date', '')
                if date_str:
                    try:
                        result_year = int(date_str.split('-')[0])
                        if result_year >= current_year - 1:
                            recent_count += 1
                    except (ValueError, IndexError):
                        continue
            
            return recent_count / len(results) if results else 0.5
        
        else:
            return 0.8  # Default for direct answers
    
    def _calculate_domain_alignment(
        self,
        query_classification: QueryClassification,
        response_data: Dict[str, Any]
    ) -> float:
        """Calculate domain alignment score."""
        # Check if response aligns with query domain
        query_keywords = set(query_classification.keywords)
        
        # Extract keywords from response
        answer = response_data.get('answer', '').lower()
        response_keywords = set()
        
        # Simple keyword matching
        domain_terms = [
            'beps', 'pillar', 'minimum tax', 'substance', 'transfer pricing',
            'oecd', 'g20', 'digital tax', 'withholding', 'treaty'
        ]
        
        for term in domain_terms:
            if term in answer:
                response_keywords.add(term)
        
        # Calculate overlap
        if not query_keywords:
            return 0.7
        
        overlap = len(query_keywords.intersection(response_keywords))
        alignment = overlap / len(query_keywords)
        
        return min(alignment + 0.3, 1.0)  # Base 0.3 for general relevance
    
    def _calculate_consistency_score(
        self,
        response_data: Dict[str, Any]
    ) -> float:
        """Calculate internal consistency score."""
        # Simple consistency check based on answer coherence
        answer = response_data.get('answer', '')
        if not answer:
            return 0.0
        
        # Check for contradictions
        contradiction_indicators = [
            'however', 'but', 'although', 'on the other hand'
        ]
        
        # Count potential contradictions
        contradiction_count = sum(1 for indicator in contradiction_indicators 
                                if indicator in answer.lower())
        
        # More contradictions = lower consistency
        if contradiction_count == 0:
            return 1.0
        elif contradiction_count == 1:
            return 0.8
        elif contradiction_count == 2:
            return 0.6
        else:
            return 0.4
    
    def _calculate_weighted_score(self, factors: ConfidenceFactors) -> float:
        """Calculate weighted overall confidence score."""
        components = {
            'query_confidence': factors.query_confidence,
            'retrieval_confidence': factors.retrieval_confidence,
            'source_reliability': factors.source_reliability,
            'answer_completeness': factors.answer_completeness,
            'temporal_relevance': factors.temporal_relevance,
            'domain_alignment': factors.domain_alignment,
            'consistency_score': factors.consistency_score
        }
        
        weighted_sum = sum(
            components[key] * self.weights[key] 
            for key in components
        )
        
        return max(0.0, min(weighted_sum, 1.0))
    
    def _generate_recommendations(
        self,
        factors: ConfidenceFactors,
        overall_score: float
    ) -> List[str]:
        """Generate recommendations based on confidence factors."""
        recommendations = []
        
        if overall_score < 0.5:
            recommendations.append("Consider rephrasing your query for better clarity")
        
        if factors.retrieval_confidence < 0.6:
            recommendations.append("Try using more specific keywords or terms")
        
        if factors.source_reliability < 0.7:
            recommendations.append("Cross-reference with official sources")
        
        if factors.answer_completeness < 0.7:
            recommendations.append("The answer may be incomplete - consider asking for more details")
        
        if factors.temporal_relevance < 0.7:
            recommendations.append("Information may be outdated - check for recent updates")
        
        if factors.domain_alignment < 0.7:
            recommendations.append("The response may not fully address your specific domain")
        
        if not recommendations and overall_score > 0.8:
            recommendations.append("High confidence response based on reliable sources")
        
        return recommendations
    
    def get_confidence_level(self, score: float) -> str:
        """Get human-readable confidence level."""
        if score >= 0.9:
            return "Very High"
        elif score >= 0.8:
            return "High"
        elif score >= 0.7:
            return "Medium-High"
        elif score >= 0.6:
            return "Medium"
        elif score >= 0.5:
            return "Medium-Low"
        elif score >= 0.4:
            return "Low"
        else:
            return "Very Low"
    
    def should_refine_query(self, confidence_score: ConfidenceScore) -> bool:
        """Determine if query should be refined based on confidence."""
        return confidence_score.overall_score < 0.6